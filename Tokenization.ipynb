{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLo0ZjbGO9rT",
        "outputId": "c601e49d-c394-4616-b151-ee4831845110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization:**\n",
        "The process of breaking down text into smaller units, called tokens, such as words, subwords, or characters, for easier processing in natural language tasks."
      ],
      "metadata": {
        "id": "w7vDXhBqUJ1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus:** A collection of texts or documents used for language processing or analysis.\n",
        "\n",
        "**Document:** A single piece of text or content within a corpus, such as an article, paragraph, or sentence.\n",
        "\n",
        "**Vocabulary:**   The unique set of words or tokens present in a corpus.\n",
        "\n",
        "**Token:** A single unit of text, such as a word, subword, or character, resulting from tokenization."
      ],
      "metadata": {
        "id": "MGtLzhvPRXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OiCqJoDCUcv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lst0xwrVTfSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "corpus = \"\"\"\n",
        "Artificial intelligence (AI) is transforming industries across the globe. From healthcare to finance, AI-powered solutions are enhancing efficiency and driving innovation. For instance, chatbots are revolutionizing customer service by providing instant support, while machine learning algorithms are uncovering patterns in data to make accurate predictions.\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "i2C9RVHHPFAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(corpus)\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp5YK52CPFDU",
        "outputId": "bd90356e-ed49-465b-b156-2a26f2f4bb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Transforming Paragraph to sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documments = sent_tokenize(corpus)\n",
        "\n",
        "for sentence in documments:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FB_7AAfPFGk",
        "outputId": "3bd8837b-cbe1-47e9-a5dd-1778fa905210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Artificial intelligence (AI) is transforming industries across the globe.\n",
            "From healthcare to finance, AI-powered solutions are enhancing efficiency and driving innovation.\n",
            "For instance, chatbots are revolutionizing customer service by providing instant support, while machine learning algorithms are uncovering patterns in data to make accurate predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming sentences to words\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "words=word_tokenize(corpus)\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIfFCUODR8Ha",
        "outputId": "4c05c327-5232-4ba1-e334-55220c81c1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial\n",
            "intelligence\n",
            "(\n",
            "AI\n",
            ")\n",
            "is\n",
            "transforming\n",
            "industries\n",
            "across\n",
            "the\n",
            "globe\n",
            ".\n",
            "From\n",
            "healthcare\n",
            "to\n",
            "finance\n",
            ",\n",
            "AI-powered\n",
            "solutions\n",
            "are\n",
            "enhancing\n",
            "efficiency\n",
            "and\n",
            "driving\n",
            "innovation\n",
            ".\n",
            "For\n",
            "instance\n",
            ",\n",
            "chatbots\n",
            "are\n",
            "revolutionizing\n",
            "customer\n",
            "service\n",
            "by\n",
            "providing\n",
            "instant\n",
            "support\n",
            ",\n",
            "while\n",
            "machine\n",
            "learning\n",
            "algorithms\n",
            "are\n",
            "uncovering\n",
            "patterns\n",
            "in\n",
            "data\n",
            "to\n",
            "make\n",
            "accurate\n",
            "predictions\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TreeBankWordTokenizer .\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "words = TreebankWordTokenizer().tokenize(corpus)\n",
        "\n",
        "for word in words:\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IUWsb4SOPFJs",
        "outputId": "5f85d262-5dcf-4ed4-9c6b-e4d8a96f8ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial\n",
            "intelligence\n",
            "(\n",
            "AI\n",
            ")\n",
            "is\n",
            "transforming\n",
            "industries\n",
            "across\n",
            "the\n",
            "globe.\n",
            "From\n",
            "healthcare\n",
            "to\n",
            "finance\n",
            ",\n",
            "AI-powered\n",
            "solutions\n",
            "are\n",
            "enhancing\n",
            "efficiency\n",
            "and\n",
            "driving\n",
            "innovation.\n",
            "For\n",
            "instance\n",
            ",\n",
            "chatbots\n",
            "are\n",
            "revolutionizing\n",
            "customer\n",
            "service\n",
            "by\n",
            "providing\n",
            "instant\n",
            "support\n",
            ",\n",
            "while\n",
            "machine\n",
            "learning\n",
            "algorithms\n",
            "are\n",
            "uncovering\n",
            "patterns\n",
            "in\n",
            "data\n",
            "to\n",
            "make\n",
            "accurate\n",
            "predictions\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        " The process of reducing a word to its base or root form by removing suffixes or prefixes, often used to normalize words in natural language processing (e.g., \"running\" â†’ \"run\")."
      ],
      "metadata": {
        "id": "sa3GznjuWORM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8e9YQE4WkTQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}